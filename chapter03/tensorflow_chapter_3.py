# -*- coding: utf-8 -*-
"""tensorflow_chapter_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1byWYbhPK4L8ZhU_mgnSEsD7jb2szscs2

기본 연산 예제 1) rank(), add(), subtract(), multiply(), divide()
"""

import tensorflow as tf
import numpy as np

a = tf.constant(3)
b = tf.constant(2)

# tf.rank() 함수
print(tf.rank(a))
print(tf.rank(b))

# 기본연산
# 텐서 형태로 출력해보기
print(tf.add(a,b))
print(tf.subtract(a,b))

# 넘파이 배열 형태로 출력해보기
print(tf.multiply(a,b).numpy()) #곱하기
print(tf.divide(a,b).numpy()) #나누기

"""기본 연산 예제 2) 행렬곱 연산"""

import tensorflow as tf
a = tf.constant([[3,4], [2,1]])
b = tf.constant([[1,5], [3,7]])
c = tf.constant([[3,5], [4,5]])
print(tf.matmul(tf.matmul(a,b), c))

d = tf.constant([[3,4,1], [2,3,4]])
e = tf.constant([[2,4], [3,5], [6,2]])
print(tf.matmul(d,e))

f = tf.constant([[3,5,3], [2,1,4], [1,2,1]])
g = tf.constant([[1,5,5], [2,3,4], [3,1,2]])
print(tf.matmul(f, g))

"""기본 연산 예제 3) 기본 상수 선언 및 Variable 선언하고 변수를 assign()으로 변경해보기"""

c = tf.constant([1.9, 2.1], dtype=tf.float32) # 상수 텐서 선언, 값 변경 불가
d = tf.cast(c, tf.int32)  # c의 원소들을 정수 데이터 타입으로 캐스팅하여 새로운 텐서 d를 생성, 원본 c는 영향 X
print(c, d)
print("\n")

a = tf.Variable([2.0, 3.0]) # 변수 텐서 a 선언, 값 변경 가능
a.assign([1,2]) #assign으로 변경함

"""기본 연산 예제 4) tf.zeros(), tf.ones(), tf.eye()"""

import tensorflow as tf

a = tf.zeros([3,4], tf.int32)
print(a)
print(a.shape)
print(a.dtype)
print("\n")

print(tf.ones([3,4], tf.int32))
print("\n")

tf.eye(4)

"""기본 연산 예제 5) tf.reshape()"""

import tensorflow as tf

t1 = [[1,2,3], [4,5,6]]
print(t1)
print("\n")

t2 = tf.reshape(t1,[6])
print(t2)
print("\n")

t3 = tf.reshape(t2, [3,2])
print(t3)

"""기본 연산 예제 6) moatplotlib.pyplot 라이브러리"""

import matplotlib.pyplot as plt

# 기본 plot
plt.plot([1,2,3,4]) # x, y 값 따로 지정 안하면 y = x 선형 그래프 출력
plt.show()  #그래프 출력

plt.plot([1,2,3,4],[1,4,9,16])
plt.show()

# plot에 x, y 라벨 붙이기
plt.plot([1,2,3,4],[2,3,5,10])
plt.xlabel('X-Axis')
plt.ylabel('Y-Axis')
# plt 보여주는 x, y 범위 지정
plt.xlim([0, 5])
plt.ylim([0, 20])
plt.show()

"""기본 연산 예제 7) GPU 체크(없으면 에러 출력)"""

import tensorflow as tf
device_name = tf.test.gpu_device_name()

if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')

print('Found GPU at : {}'.format(device_name))

"""기본 연산 예제 8) 텐서에서 넘파이, 넘파이에서 텐서
*   numpy()와 convert_to_tensor() 함수
*   tensor와 numpy array간 변환이 매우 유연



"""

import tensorflow as tf
import numpy as np

a = tf.constant(1)
b = tf.constant(4)
c = tf.add(a, b).numpy() # a와 b를 더한 후 Numpy 배열 형태로 변환
c_square = np.square(c, dtype = np.float32)
# Numpy 모듈에 존재하는 square 함수 적용
c_tensor = tf.convert_to_tensor(c_square) # 다시 텐서로 변환

# 넘파이 배열과 텐서 각각을 확인하기 위해 출력
print("numpy array : %0.1f, applying square with numpy : %0.1f, convert_to_tensor : %0.1f" % (c,c_square, c_tensor))

"""신경망 예제 1) 키와 몸무게 예측 (Linear Regression)"""

import tensorflow as tf

# 변수 초기화
height = 170
shoe_size = 260

# 회귀 계수 (a)와 절편 (b)를 텐서플로 변수로 선언
a = tf.Variable(0.1)
b = tf.Variable(0.2)

# 손실 함수 정의
def loss_function():
  predicted_value = height * a + b
  return tf.square(shoe_size - predicted_value) # 실제 값과 예측 값의 차이의 제곱을 반환

# 옵티마이저 설정
opt = tf.keras.optimizers.Adam(learning_rate = 0.1)

# 최적화 과정 수행
for i in range(300):
  # 손실함수를 최소화하는 방향으로 a와 b를 업데이트
  opt.minimize(loss_function, var_list = [a, b])
  if i % 20 == 0: # 20번의 반복마다 한 번씩 a, b 값 출력
    print(f"Step {i}: a = {a.numpy()}, b = {b.numpy()}")

"""신경망 예제 2) 리스트 훈련데이터 (Linear Regression)"""

import tensorflow as tf

# 리스트 훈련데이터
train_x = [1, 2, 3, 4, 5, 6, 7]
train_y = [3, 5, 7, 9, 11, 13, 15]

# 모델 파라미터 초기화 (기울기 a와 절편 b)
a = tf.Variable(0.1)
b = tf.Variable(0.1)

def loss_function():
  predicted_y = train_x * a + b
  return tf.keras.losses.mse(train_y, predicted_y)

# 최적화 설정
opt1 = tf.keras.optimizers.Adam(learning_rate = 0.1)
opt2 = tf.keras.optimizers.Adam(learning_rate = 0.001)

for i in range(300):
  opt1.minimize(loss_function, var_list = [a, b])
  print(a.numpy(), b.numpy())

print("\n")

for i in range(300):
  opt2.minimize(loss_function, var_list = [a, b])
  print(a.numpy(), b.numpy())

"""신경망 예제 3) 퍼셉트론 OR 게이트"""

import tensorflow as tf
tf.random.set_seed(777) # 시드 설정 (실험의 재생산성)
# 시드 설정함으로써 재현 가능한 결과 얻기 위
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from keras.losses import mse

# 데이터 준비
x = np.array([[0,0], [1,0], [0,1], [1,1]])
y = np.array([[0], [1], [1], [1]])

# 모델 구성
model = Sequential()
# 단층 퍼셉트론을 구성
model.add(Dense(1, input_shape=(2,), activation = 'linear'))

# 모델 준비
model.compile(optimizer=SGD(), loss = mse, metrics = ['acc'])  # list 형태로 평가지표를 전달
              # 성능 가장 좋은 optimizer 찾고, loss를 최소화 시키기(출력), acc는 정확도(출력)

# 학습시키기
model.fit(x, y, epochs = 500)

"""신경망 예제 4) 퍼셉트론 AND 게이트"""

import tensorflow as tf
tf.random.set_seed(777) # 시드 설정 (실험의 재생산성)
# 시드 설정함으로써 재현 가능한 결과 얻기 위
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from keras.losses import mse

# 데이터 준비
x = np.array([[0,0], [1,0], [0,1], [1,1]])
y = np.array([[0], [0], [0], [1]])

# 모델 구성
model = Sequential()
# 단층 퍼셉트론을 구성
model.add(Dense(1, input_shape=(2,), activation = 'linear'))

# 모델 준비
model.compile(optimizer=SGD(), loss = mse, metrics = ['acc'])  # list 형태로 평가지표를 전달
              # 성능 가장 좋은 optimizer 찾고, loss를 최소화 시키기(출력), acc는 정확도(출력)

# 학습시키기
model.fit(x, y, epochs = 500)

"""신경망 예제 5) 퍼셉트론 NAND 게이트"""

import tensorflow as tf
tf.random.set_seed(777) # 시드 설정 (실험의 재생산성)
# 시드 설정함으로써 재현 가능한 결과 얻기 위
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from keras.losses import mse

# 데이터 준비
x = np.array([[0,0], [1,0], [0,1], [1,1]])
y = np.array([[1], [1], [1], [0]])

# 모델 구성
model = Sequential()
# 단층 퍼셉트론을 구성
model.add(Dense(1, input_shape=(2,), activation = 'linear'))

# 모델 준비
model.compile(optimizer=SGD(), loss = mse, metrics = ['acc'])  # list 형태로 평가지표를 전달
              # 성능 가장 좋은 optimizer 찾고, loss를 최소화 시키기(출력), acc는 정확도(출력)

# 학습시키기
model.fit(x, y, epochs = 500)

"""신경망 예제 6) XOR 게이트는 퍼셉트론으로 해결할 수 없다(확실히 구분할 수 없다)"""

import tensorflow as tf
tf.random.set_seed(777) # 시드 설정 (실험의 재생산성)
# 시드 설정함으로써 재현 가능한 결과 얻기 위
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from keras.losses import mse

# 데이터 준비
x = np.array([[0,0], [1,0], [0,1], [1,1]])
y = np.array([[0], [1], [1], [0]])

# 모델 구성
model = Sequential()
# 단층 퍼셉트론을 구성
model.add(Dense(1, input_shape=(2,), activation = 'linear'))

# 모델 준비
model.compile(optimizer=SGD(), loss = mse, metrics = ['acc'])  # list 형태로 평가지표를 전달
              # 성능 가장 좋은 optimizer 찾고, loss를 최소화 시키기(출력), acc는 정확도(출력)

# 학습시키기
model.fit(x, y, epochs = 500)

"""신경망 예제 7) 다층 퍼셉트론 - XOR 게이트"""

import tensorflow as tf
tf.random.set_seed(777) # 시드 설정 (실험의 재생산성)
# 시드 설정함으로써 재현 가능한 결과 얻기 위
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import RMSprop
from keras.losses import mse

data = np.array([[0,0], [1,0], [0,1], [1,1]])
label = np.array([[0], [1], [1], [0]])

# 모델 구성
model = Sequential()
# 다층 퍼셉트론을 구성
model.add(Dense(32, input_shape = (2,), activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid'))

# 모델 준비
model.compile(optimizer=RMSprop(), loss = mse, metrics = ['acc'])  # list 형태로 평가지표를 전달
              # 성능 가장 좋은 optimizer 찾고, loss를 최소화 시키기(출력), acc는 정확도(출력)

# 학습시키기
model.fit(x, y, epochs = 1000)

"""활성화 함수 예제 1) 활성화 함수 구현해보기

*   sigmoid()
*   tanh()
*   relu()
*   ricky_relu()







"""

import numpy as np
import matplotlib.pyplot as plt
import math

def sigmoid(x):
  return 1/(1+np.exp(-x))

def tanh(x):
  return list(map(lambda x : math.tanh(x),x))

def relu(x):
  result = []
  for ele in x:
    if(ele <= 0):
      result.append(0)
    else:
      result.append(ele)
  return result

def leaky_relu(x, alpha=0.1):
  return [max(ele, alpha * ele) for ele in x]

x = np.linspace(-4, 4, 100)
# print(x)
sig = sigmoid(x)
th = tanh(x)
rl = relu(x)
leaky = leaky_relu(x)
plt.plot(x, sig, color = 'red', label = 'sigmoid')
plt.plot(x, th, color = 'yellow', label = 'tanh')
plt.plot(x, rl, color = 'blue', label = 'relu')
plt.plot(x, leaky, color = 'green', label = 'Leaky_relu')
plt.legend()
plt.show()

"""경사하강법 예제 1) 경사하강법 구현해보기"""

# 경사하강법 실습
import numpy as np
import matplotlib.pyplot as plt

lr_list = [0.001, 0.01, 0.1, 0.2, 0.5, 0.9]

def get_derivative(lr):
  w_old = 2
  derivative = [w_old]
  y = [w_old ** 2]
  for i in range(1,10):
    dev_value = w_old * 2
    w_new = w_old - lr * dev_value
    w_old = w_new
    derivative.append(w_old)
    y.append(w_old ** 2)
  return derivative, y

x = np.linspace(-2, 2, 50)
x_square = [i**2 for i in x]
fig = plt.figure(figsize=(9,5))
for i, lr in enumerate(lr_list):
  derivative, y = get_derivative(lr)
  ax = fig.add_subplot(2,3,i+1)
  ax.scatter(derivative, y, color='red')
  ax.plot(x,x_square)
  ax.title.set_text('lr =' + str(lr))
  plt.xlabel('weight')
  plt.ylabel('loss')

plt.tight_layout()
plt.show()

"""태운_추가) 사이킷런을 이용한 확률적 경사 하강법 적용"""

import pandas as pd # 판다스 데이터 프레임
from sklearn.model_selection import train_test_split # 세트 나누기 위한 모듈
from sklearn.preprocessing import StandardScaler # 전처리 모듈
from sklearn.linear_model import SGDClassifier # 확률적 경사 하강법 모듈
fish = pd.read_csv('https://bit.ly/fish_csv_data')

fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()


train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)


sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

sc.partial_fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))